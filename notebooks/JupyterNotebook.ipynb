{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yfinance import download\n",
    "\n",
    "tickers = [\"MSFT\",\"AAPL\",\"NVDA\",\"GOOG\",\"AMZN\",\"2222.SR\",\"META\",\"TSM\",\"BRK-B\",\"LLY\",\"AVGO\",\"NVO\",\"TSLA\",\"V\",\"JPM\",\"WMT\",\n",
    "            \"XOM\",\"TCEHY\",\"UNH\",\"MA\",\"ASML\",\"PG\",\"ORCL\",\"MC.PA\",\"005930.KS\",\"COST\",\"JNJ\",\"HD\",\"MRK\",\"BAC\",\"ABBV\",\"NFLX\",\n",
    "            \"CVX\",\"NESN.SW\",\"KO\",\"TM\",\"AMD\",\"600519.SS\",\"1398.HK\",\"OR.PA\",\"AZN\",\"601857.SS\",\"RMS.PA\",\"QCOM\",\n",
    "            \"CRM\",\"ADBE\",\"RELIANCE.NS\",\"PEP\",\"ROG.SW\", \"SAP\"]\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2024-01-01'\n",
    "for ticker in tickers:\n",
    "    dataframe = download(ticker, start=start_date, end=end_date)\n",
    "    filePath = f'C:\\\\Users\\\\samim\\\\OneDrive\\\\Documents\\\\Projects\\\\FinancialModelingTool\\\\data\\\\{ticker}_historical_data.csv'\n",
    "    dataframe.to_csv(filePath)\n",
    "    print(f\"Data saved to data/{ticker}_historical_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and Preprocessing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "def inspect_data(data):\n",
    "    print(data.head())\n",
    "    print(data.info())\n",
    "    print(data.describe())\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\samim\\\\OneDrive\\\\Documents\\\\Projects\\\\FinancialModelingTool\\\\data'\n",
    "for filename in listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Load and inspect data\n",
    "        filePath = join(folder_path, filename)\n",
    "        data = read_csv(filePath, index_col='Date', parse_dates=True)\n",
    "        inspect_data(data)\n",
    "\n",
    "        # Handle missing values\n",
    "        data_cleaned = data.dropna()\n",
    "        data_cleaned.to_csv(filePath)\n",
    "        inspect_data(data_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, to_datetime\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "def inspect_data(data):\n",
    "    print(data.head())\n",
    "    print(data.info())\n",
    "    print(data.describe())\n",
    "\n",
    "def feature_engineering(data):\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    data.index = to_datetime(data.index)\n",
    "\n",
    "    # Determine the minimum length of data to apply the largest window\n",
    "    min_length = len(data)\n",
    "\n",
    "    # Moving Averages\n",
    "    windows = {\n",
    "        '1_day_MA': 1, '3_day_MA': 3, '5_day_MA': 5, '7_day_MA': 7, '14_day_MA': 14,\n",
    "        '1_month_MA': 30, '3_month_MA': 91, '6_month_MA': 183, '1_year_MA': 365\n",
    "    }\n",
    "    for key, window in windows.items():\n",
    "        if min_length >= window:\n",
    "            data[key] = data['Close'].rolling(window=window).mean()\n",
    "\n",
    "    # Daily Returns\n",
    "    data['Daily_Return'] = data['Close'].pct_change() * 100\n",
    "\n",
    "    # Volatility\n",
    "    vol_windows = {\n",
    "        'Volatility_1_day': 1, 'Volatility_3_day': 3, 'Volatility_5_day': 5, 'Volatility_7_day': 7,\n",
    "        'Volatility_14_day': 14, 'Volatility_1_month': 30, 'Volatility_3_month': 91,\n",
    "        'Volatility_6_month': 183, 'Volatility_1_year': 365\n",
    "    }\n",
    "    for key, window in vol_windows.items():\n",
    "        if min_length >= window:\n",
    "            data[key] = data['Daily_Return'].rolling(window=window).std()\n",
    "\n",
    "    # High-Low Difference\n",
    "    data['High_Low_Diff'] = data['High'] - data['Low']\n",
    "\n",
    "    # Lagged Features\n",
    "    lags = [1, 3, 5]\n",
    "    for lag in lags:\n",
    "        data[f'Lag_{lag}'] = data['Close'].shift(lag)\n",
    "\n",
    "    # Rolling Statistics\n",
    "    roll_windows = {\n",
    "        'Rolling_Std_1_day': 1, 'Rolling_Std_3_day': 3, 'Rolling_Std_5_day': 5, 'Rolling_Std_7_day': 7,\n",
    "        'Rolling_Std_14_day': 14, 'Rolling_Std_1_month': 30, 'Rolling_Std_3_month': 91,\n",
    "        'Rolling_Std_6_month': 183, 'Rolling_Std_1_year': 365\n",
    "    }\n",
    "    for key, window in roll_windows.items():\n",
    "        if min_length >= window:\n",
    "            data[key] = data['Close'].rolling(window=window).std()\n",
    "\n",
    "    # Calendar Features\n",
    "    data['Day_of_Week'] = data.index.dayofweek\n",
    "    data['Month'] = data.index.month\n",
    "    data['Quarter'] = data.index.quarter\n",
    "\n",
    "    return data\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\samim\\\\OneDrive\\\\Documents\\\\Projects\\\\FinancialModelingTool\\\\data'\n",
    "for filename in listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Load and inspect data\n",
    "        filePath = join(folder_path, filename)\n",
    "        data = read_csv(filePath, index_col='Date', parse_dates=True)\n",
    "        inspect_data(data)\n",
    "\n",
    "        data_with_features = feature_engineering(data)\n",
    "        data_with_features.to_csv(filePath)\n",
    "        inspect_data(data_with_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv, to_numeric\n",
    "from numpy import array, exp, nan, reshape, zeros\n",
    "from prophet import Prophet\n",
    "from lightgbm import LGBMRegressor\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from json import dump\n",
    "\n",
    "# Map intervals to human-readable labels\n",
    "interval_labels = {\n",
    "    '1_days': 1,\n",
    "    '3_days': 3,\n",
    "    '5_days': 5,\n",
    "    '7_days': 7,\n",
    "    '14_days': 14,\n",
    "    '30_days': 30,\n",
    "    '91_days': 91,\n",
    "    '183_days': 183,\n",
    "    '365_days': 365\n",
    "}\n",
    "\n",
    "def load_data(filePath):\n",
    "    data = read_csv(filePath, index_col='Date', parse_dates=True)\n",
    "    return data\n",
    "\n",
    "def linear_regression_forecast(data, intervals):\n",
    "    data['Date'] = data.index\n",
    "    data['Date'] = to_numeric(data['Date'])\n",
    "\n",
    "    x = data[['Date']].values\n",
    "    y = data['Close'].values\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    poly = PolynomialFeatures(degree=3) # Using degree 3 for polynomial features\n",
    "    model = make_pipeline(poly, LinearRegression())\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    predictions = {}\n",
    "    for interval in intervals:\n",
    "        future_dates = array([x_test[-1][0] + i for i in range(1, interval+2)]).reshape(-1, 1)\n",
    "        predictions[f'{interval}_days'] = model.predict(future_dates)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def arima_forecast(data, intervals):\n",
    "    data = data['Close']\n",
    "    train_data, test_data = data[:int(len(data)*0.8)], data[int(len(data)*0.8):]\n",
    "    \n",
    "    model = ARIMA(train_data, order=(5, 1, 2))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    predictions = {}\n",
    "    for interval in intervals:\n",
    "        y_pred = model_fit.forecast(steps=interval+1)\n",
    "        predictions[f'{interval}_days'] = y_pred.values\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_lstm_model(data, feature_col='Close', n_steps=30):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data[[feature_col]].values)\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(n_steps, len(data_scaled)):\n",
    "        X.append(data_scaled[i-n_steps:i, 0])\n",
    "        y.append(data_scaled[i, 0])\n",
    "\n",
    "    X, y = array(X), array(y)\n",
    "    X = reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X, y, epochs=25, batch_size=32, verbose=2)\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "def predict_lstm(model, scaler, data, n_steps=30, intervals=[1]):\n",
    "    inputs = data['Close'][-n_steps:].values\n",
    "    inputs = inputs.reshape(-1, 1)\n",
    "    inputs = scaler.transform(inputs)\n",
    "    inputs = reshape(inputs, (1, n_steps, 1))\n",
    "\n",
    "    predictions = {}\n",
    "    for interval in intervals:\n",
    "        future_inputs = zeros((1, n_steps + interval + 1, 1))\n",
    "        future_inputs[:, :n_steps, :] = inputs\n",
    "        for i in range(interval + 1):\n",
    "            future_inputs[:, n_steps + i, :] = model.predict(future_inputs[:, i:i + n_steps, :])\n",
    "        predicted_price = scaler.inverse_transform(future_inputs[:, n_steps:, :].reshape(-1, 1))\n",
    "        predictions[f'{interval}_days'] = predicted_price.flatten()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def create_prophet_model(data):\n",
    "    df = data.copy()\n",
    "    df = df.rename_axis('ds').reset_index()\n",
    "    df = df.rename(columns={'Close': 'y'})\n",
    "    \n",
    "    model = Prophet(daily_seasonality=True, yearly_seasonality=True)\n",
    "    model.fit(df)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_prophet(model, periods):\n",
    "    future = model.make_future_dataframe(periods=periods[-1] + 1)\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    predictions = {}\n",
    "    for period in periods:\n",
    "        predictions[f'{period}_days'] = forecast['yhat'].values[-(period+1):]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def create_rf_model(data, feature_cols, target_col='Close'):\n",
    "    X = data[feature_cols]\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_rf(model, data, feature_cols, intervals):\n",
    "    predictions = {}\n",
    "    X_future = data[feature_cols].tail(intervals[-1]+1)\n",
    "    for interval in intervals:\n",
    "        predictions[f'{interval}_days'] = model.predict(X_future[-(interval+1):])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_xgb_model(data, feature_cols, target_col='Close'):\n",
    "    X = data[feature_cols]\n",
    "    y = data[target_col]\n",
    "\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_xgb(model, data, feature_cols, intervals):\n",
    "    predictions = {}\n",
    "    X_future = data[feature_cols].tail(intervals[-1]+1)\n",
    "    for interval in intervals:\n",
    "        predictions[f'{interval}_days'] = model.predict(X_future[-(interval+1):])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# SVR with RBF Kernal\n",
    "def create_svr_model(data, feature_cols, target_col='Close'):\n",
    "    X = data[feature_cols]\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    model = SVR(kernel='rbf', C=100, gamma=0.1) # Using RBF Kernal with adjusted parameters\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_svr(model, data, feature_cols, intervals):\n",
    "    predictions = {}\n",
    "    X_future = data[feature_cols].tail(intervals[-1]+1)\n",
    "    for interval in intervals:\n",
    "        predictions[f'{interval}_days'] = model.predict(X_future[-(interval+1):])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_sarima_model(data, order=(1,1,1), seasonal_order=(1,1,1,12)):\n",
    "    model = SARIMAX(data['Close'], order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    model_fit = model.fit(disp=False)\n",
    "\n",
    "    return model_fit\n",
    "\n",
    "def predict_sarima(model_fit, steps):\n",
    "    predictions = {}\n",
    "    for step in steps:\n",
    "        forecast = model_fit.forecast(steps=step+1)\n",
    "        predictions[f'{step}_days'] = forecast.values\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def create_gbm_model(data, feature_cols, target_col='Close'):\n",
    "    X = data[feature_cols]\n",
    "    y = data[target_col]\n",
    "\n",
    "    model = LGBMRegressor()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_gbm(model, data, feature_cols, intervals):\n",
    "    predictions = {}\n",
    "    X_future = data[feature_cols].tail(intervals[-1]+1)\n",
    "    for interval in intervals:\n",
    "        predictions[f'{interval}_days'] = model.predict(X_future[-(interval+1):])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_knn_model(data, feature_cols, target_col='Close'):\n",
    "    X = data[feature_cols]\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_knn(model, data, feature_cols, intervals):\n",
    "    predictions = {}\n",
    "    X_future = data[feature_cols].tail(intervals[-1]+1)\n",
    "    for interval in intervals:\n",
    "        predictions[f'{interval}_days'] = model.predict(X_future[-(interval+1):])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def calculate_accuracy_percentage(predictions, actual_prices):\n",
    "    percentage_differences = abs(predictions - actual_prices) / actual_prices * 100\n",
    "    scaled_differences = exp(-percentage_differences / 5)  # exponential decay scaling\n",
    "    weighted_accuracy = scaled_differences.mean() * 100\n",
    "    return weighted_accuracy\n",
    "\n",
    "def save_forecasts(predictions, actual_prices, data, company_name, accuracy_metrics):\n",
    "    results = []\n",
    "    for model_name, model_preds in predictions.items():\n",
    "        for interval, preds in model_preds.items():\n",
    "            interval_length = interval_labels[interval]\n",
    "            actual_interval_prices = actual_prices[-(interval_length + 1):]\n",
    "            accuracy = calculate_accuracy_percentage(preds, actual_interval_prices)\n",
    "            accuracy_metrics[model_name][interval].append(accuracy)\n",
    "            for i, pred in enumerate(preds):\n",
    "                actual_price_index = len(data) - (interval_labels[interval]) + i - 1\n",
    "                actual_price = actual_prices[actual_price_index] if actual_price_index >= 0 else nan\n",
    "                results.append([company_name, model_name, interval, i, pred, actual_price, accuracy])\n",
    "    df_results = DataFrame(results, columns=['Company', 'Model','Interval', 'Day', 'Prediction', 'Actual', 'Accuracy'])\n",
    "    df_results.to_csv(f'forecast_data/{company_name}_forecasts.csv', index=False)\n",
    "\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\samim\\\\OneDrive\\\\Documents\\\\Projects\\\\FinancialModelingTool\\\\data'\n",
    "feature_cols = ['7_day_MA', '14_day_MA', 'Volume', 'Daily_Return', 'Volatility_7_day', 'High_Low_Diff']\n",
    "intervals = [1, 3, 5, 7, 14, 30, 91, 183, 365]  # 1 day, 3 days, 5 days, 1 week, 2 weeks, 1 month, 3 months, 6 months, 1 year\n",
    "\n",
    "accuracy_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for _ in range(10):\n",
    "    for filename in listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Load and append data to all_data\n",
    "            filePath = join(folder_path, filename)\n",
    "            company_name = filename.split('_')[0]\n",
    "            data = load_data(filePath)\n",
    "            \n",
    "            predictions = {}\n",
    "            actual_prices = data['Close'].values\n",
    "\n",
    "            # Linear Regression Forecast\n",
    "            predictions['Linear Regression'] = linear_regression_forecast(data, intervals)\n",
    "\n",
    "            # ARIMA Forecast\n",
    "            predictions['ARIMA'] = arima_forecast(data, intervals)\n",
    "\n",
    "            # LSTM Forecast\n",
    "            lstm_model, lstm_scaler = create_lstm_model(data)\n",
    "            predictions['LSTM'] = predict_lstm(lstm_model, lstm_scaler, data, intervals=intervals)\n",
    "\n",
    "            # Prophet Forecast\n",
    "            prophet_model = create_prophet_model(data)\n",
    "            predictions['Prophet'] = predict_prophet(prophet_model, intervals)\n",
    "\n",
    "            # Random Forest Forecast\n",
    "            rf_model = create_rf_model(data, feature_cols)\n",
    "            predictions['Random Forest'] = predict_rf(rf_model, data, feature_cols, intervals)\n",
    "\n",
    "            # XGBoost Forecast\n",
    "            xgb_model = create_xgb_model(data, feature_cols)\n",
    "            predictions['XGBoost'] = predict_xgb(xgb_model, data, feature_cols, intervals)\n",
    "\n",
    "            # SVR Forecast\n",
    "            svr_model = create_svr_model(data, feature_cols)\n",
    "            predictions['SVR'] = predict_svr(svr_model, data, feature_cols, intervals)\n",
    "\n",
    "            # SARIMA Forecast\n",
    "            sarima_model_fit = create_sarima_model(data)\n",
    "            predictions['SARIMA'] = predict_sarima(sarima_model_fit, intervals)\n",
    "\n",
    "            # GBM Forecast\n",
    "            gbm_model = create_gbm_model(data, feature_cols)\n",
    "            predictions['GBM'] = predict_gbm(gbm_model, data, feature_cols, intervals)\n",
    "\n",
    "            # KNN Forecast\n",
    "            knn_model = create_knn_model(data, feature_cols)\n",
    "            predictions['KNN'] = predict_knn(knn_model, data, feature_cols, intervals)\n",
    "\n",
    "            # Save Forecasts\n",
    "            save_forecasts(predictions, actual_prices, data, company_name, accuracy_metrics)\n",
    "\n",
    "# Calculate average accuracy metrics\n",
    "average_accuracy_metrics = {model: {interval: sum(acc_list)/len(acc_list) for interval, acc_list in intervals_dict.items()} for model, intervals_dict in accuracy_metrics.items()}\n",
    "\n",
    "# Save average accuracy metrics\n",
    "with open('average_accuracy_metrics.json', 'w') as f:\n",
    "    dump(average_accuracy_metrics, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
